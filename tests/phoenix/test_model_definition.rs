// Integration test for Model Definition scenario from quickstart.md
// This test validates that users can define models with PyTorch-like syntax

#[cfg(feature = "torch-rs")]
mod model_definition_integration {
    use tch::{nn, Device, Tensor, Kind};

    // This test will fail until nn::Module derive macro is implemented
    // It validates the first quickstart scenario

    #[derive(Debug)] // nn::Module will be added when derive macro exists
    struct MLP {
        fc1: TestLinear,
        fc2: TestLinear,
        fc3: TestLinear,
        dropout: TestDropout,
    }

    impl MLP {
        fn new() -> Self {
            Self {
                fc1: TestLinear::new(784, 256),
                fc2: TestLinear::new(256, 128),
                fc3: TestLinear::new(128, 10),
                dropout: TestDropout::new(0.2),
            }
        }

        // This will be generated by derive macro in the future
        fn forward(&self, xs: &Tensor) -> Tensor {
            xs.apply(&self.fc1)
              .relu()
              .apply(&self.dropout)
              .apply(&self.fc2)
              .relu()
              .apply(&self.dropout)
              .apply(&self.fc3)
        }

        // These methods should be generated automatically by derive macro
        fn parameters(&self) -> Vec<&Tensor> {
            let mut params = Vec::new();
            params.extend(self.fc1.parameters());
            params.extend(self.fc2.parameters());
            params.extend(self.fc3.parameters());
            params
        }

        fn named_parameters(&self) -> std::collections::HashMap<String, &Tensor> {
            let mut named_params = std::collections::HashMap::new();

            for (name, param) in self.fc1.named_parameters() {
                named_params.insert(format!("fc1.{}", name), param);
            }
            for (name, param) in self.fc2.named_parameters() {
                named_params.insert(format!("fc2.{}", name), param);
            }
            for (name, param) in self.fc3.named_parameters() {
                named_params.insert(format!("fc3.{}", name), param);
            }

            named_params
        }

        fn to_device(&mut self, _device: Device) -> Result<(), String> {
            // This should be generated by derive macro
            // For now, just return Ok
            Ok(())
        }
    }

    // Temporary test implementations - will be replaced by real layers
    struct TestLinear {
        weight: Tensor,
        bias: Tensor,
        in_features: i64,
        out_features: i64,
    }

    impl TestLinear {
        fn new(in_features: i64, out_features: i64) -> Self {
            Self {
                weight: Tensor::randn(&[out_features, in_features], (Kind::Float, Device::Cpu))
                    .set_requires_grad(true),
                bias: Tensor::randn(&[out_features], (Kind::Float, Device::Cpu))
                    .set_requires_grad(true),
                in_features,
                out_features,
            }
        }

        fn apply(&self, input: &Tensor) -> Tensor {
            input.matmul(&self.weight.tr()) + &self.bias
        }

        fn parameters(&self) -> Vec<&Tensor> {
            vec![&self.weight, &self.bias]
        }

        fn named_parameters(&self) -> std::collections::HashMap<String, &Tensor> {
            let mut params = std::collections::HashMap::new();
            params.insert("weight".to_string(), &self.weight);
            params.insert("bias".to_string(), &self.bias);
            params
        }
    }

    struct TestDropout {
        p: f64,
    }

    impl TestDropout {
        fn new(p: f64) -> Self {
            Self { p }
        }

        fn apply(&self, input: &Tensor) -> Tensor {
            // For testing, just return input (no actual dropout)
            input.copy()
        }
    }

    #[test]
    fn test_model_definition_syntax() {
        // This test validates that model definition feels natural
        let model = MLP::new();

        // Should be able to create model without verbose boilerplate
        assert_eq!(model.parameters().len(), 6); // 3 layers Ã— 2 params each
    }

    #[test]
    fn test_automatic_parameter_discovery() {
        let model = MLP::new();

        // Parameters should be discovered automatically
        let params = model.parameters();
        assert_eq!(params.len(), 6);

        // All parameters should require gradients
        for param in &params {
            assert!(param.requires_grad());
        }
    }

    #[test]
    fn test_named_parameter_hierarchy() {
        let model = MLP::new();

        let named_params = model.named_parameters();
        assert_eq!(named_params.len(), 6);

        // Should follow PyTorch naming conventions
        assert!(named_params.contains_key("fc1.weight"));
        assert!(named_params.contains_key("fc1.bias"));
        assert!(named_params.contains_key("fc2.weight"));
        assert!(named_params.contains_key("fc2.bias"));
        assert!(named_params.contains_key("fc3.weight"));
        assert!(named_params.contains_key("fc3.bias"));
    }

    #[test]
    fn test_forward_pass() {
        let model = MLP::new();
        let input = Tensor::randn(&[1, 784], (Kind::Float, Device::Cpu));

        let output = model.forward(&input);

        // Should produce correct output shape
        assert_eq!(output.size(), &[1, 10]);
    }

    #[test]
    fn test_device_movement() {
        let mut model = MLP::new();

        // Should be able to move to device (even if it's the same device)
        let result = model.to_device(Device::Cpu);
        assert!(result.is_ok());
    }

    #[test]
    fn test_model_structure_inspection() {
        let model = MLP::new();

        // Should be able to inspect model structure
        let named_params = model.named_parameters();

        // Verify layer dimensions are correct
        let fc1_weight = named_params.get("fc1.weight").unwrap();
        assert_eq!(fc1_weight.size(), &[256, 784]);

        let fc2_weight = named_params.get("fc2.weight").unwrap();
        assert_eq!(fc2_weight.size(), &[128, 256]);

        let fc3_weight = named_params.get("fc3.weight").unwrap();
        assert_eq!(fc3_weight.size(), &[10, 128]);
    }

    #[test]
    fn test_gradient_flow() {
        let model = MLP::new();
        let input = Tensor::randn(&[1, 784], (Kind::Float, Device::Cpu));

        let output = model.forward(&input);
        let loss = output.sum(Kind::Float);

        // Should be able to compute gradients
        loss.backward();

        // Parameters should have gradients after backward pass
        for param in model.parameters() {
            if param.requires_grad() {
                assert!(param.grad().is_some(), "Parameter should have gradient");
            }
        }
    }

    #[test]
    fn test_multiple_forward_passes() {
        let model = MLP::new();

        // Should handle multiple forward passes
        for _ in 0..5 {
            let input = Tensor::randn(&[1, 784], (Kind::Float, Device::Cpu));
            let output = model.forward(&input);
            assert_eq!(output.size(), &[1, 10]);
        }
    }

    #[test]
    fn test_batch_processing() {
        let model = MLP::new();

        // Should handle different batch sizes
        let batch_sizes = vec![1, 2, 4, 8, 16];

        for batch_size in batch_sizes {
            let input = Tensor::randn(&[batch_size, 784], (Kind::Float, Device::Cpu));
            let output = model.forward(&input);
            assert_eq!(output.size(), &[batch_size, 10]);
        }
    }

    #[test]
    #[should_panic(expected = "Parameter should have gradient")]
    fn test_gradient_requirement_validation() {
        // This test should fail until proper gradient handling is implemented
        let model = MLP::new();
        let input = Tensor::randn(&[1, 784], (Kind::Float, Device::Cpu));

        let output = model.forward(&input);
        let loss = output.sum(Kind::Float);
        loss.backward();

        // This will panic until proper gradient handling exists
        for param in model.parameters() {
            if param.requires_grad() {
                assert!(param.grad().is_some(), "Parameter should have gradient");
            }
        }
    }
}

#[cfg(not(feature = "torch-rs"))]
mod disabled_tests {
    #[test]
    #[ignore]
    fn model_definition_tests_require torch-rs feature() {
        panic!("Model definition integration tests require 'torch-rs' feature to be enabled");
    }
}